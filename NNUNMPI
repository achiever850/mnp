import sys
import logging
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import col, to_timestamp
from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, FloatType, BooleanType, TimestampType

# Initialize logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Function to load configuration from environment variables or AWS Systems Manager Parameter Store
def load_config():
    # Here, you would typically fetch these from AWS Parameter Store or Secrets Manager
    return {
        "S3_PATH": "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/",
        "REDSHIFT_TEMP_DIR": "s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/",
        "REDSHIFT_DATABASE": "hcd-dev-db",
    }

# Initialize Glue context and job
args = getResolvedOptions(sys.argv, ["JOB_NAME"])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args["JOB_NAME"], args)

config = load_config()
config['REDSHIFT_SCHEMA'] = "usastaffing_staging"

# Define schema for announcements table matching Redshift DDL
announcements_schema = StructType([
    StructField("tenantid", IntegerType(), False),  # NOT NULL in Redshift
    StructField("announcementId", LongType(), False),  # NOT NULL in Redshift
    StructField("announcementNumber", StringType(), False),  # NOT NULL in Redshift
    StructField("announcementStatus", StringType(), True),
    StructField("announcementTemplate", StringType(), True),
    StructField("salaryType", StringType(), True),
    StructField("minimumSalary", FloatType(), True),
    StructField("maximumSalary", FloatType(), True),
    StructField("notToExceed", StringType(), True),
    StructField("totalOpenings", StringType(), True),
    StructField("openDate", TimestampType(), True),
    StructField("closeDate", TimestampType(), True),
    StructField("releasedFlag", BooleanType(), True),
    StructField("releaseddatetime", TimestampType(), True),
    StructField("applicationLimitSetFlag", BooleanType(), True),
    StructField("applicationLimit", IntegerType(), True),
    StructField("displayDefaultBenefitsTextFlag", BooleanType(), True),
    StructField("externalContactId", LongType(), True),
    StructField("externalContactName", StringType(), True),
    StructField("externalContactEmail", StringType(), True),
    StructField("internalContactId", LongType(), True),
    StructField("internalContactName", StringType(), True),
    StructField("internalContactEmail", StringType(), True),
    StructField("usajobsControlNumber", LongType(), True),
    StructField("linkedUSAJOBSCONTROLNUMBER", StringType(), True),
    StructField("WhoMayApply", StringType(), True),
    StructField("whoMayApplyOverrideText", StringType(), True),
    StructField("promotionpotential", StringType(), True),
    StructField("usajobstatus", StringType(), True),
    StructField("lastmodifieddatetime", TimestampType(), True),
    StructField("dwLastModifieddatetime", TimestampType(), True)
])

# List of New Folders (and corresponding Redshift tables) to process
folders = [
    ("announcements", "announcement"),
    # Add other folders if required
]

def process_folder(folder, redshift_table, schema):
    try:
        folder_path = f"{config['S3_PATH']}{folder}/"
        logger.info(f"Processing Folder: {folder} â†’ Redshift Table: {redshift_table}")
        logger.info(f"Full S3 Folder Path: {folder_path}")

        # Read CSV from S3
        df = spark.read.csv(folder_path, header=True, inferSchema=True)

        # Handle missing data for mandatory fields
        df = df.fillna({"tenantid": 0, "announcementId": 0, "announcementNumber": ""})

        # Apply schema
        for field in schema.fields:
            if isinstance(field.dataType, TimestampType):
                df = df.withColumn(field.name, to_timestamp(col(field.name)))
            else:
                df = df.withColumn(field.name, col(field.name).cast(field.dataType))

        logger.info(f"Schema for {folder}:")
        df.printSchema()
        logger.info(f"Sample Data from {folder}:")
        df.show(5)

        # Convert DataFrame to DynamicFrame
        dynamic_frame = DynamicFrame.fromDF(df, glueContext, "dynamic_frame")

        # Truncate the Redshift table before loading new data
        truncate_query = f"TRUNCATE TABLE {config['REDSHIFT_SCHEMA']}.{redshift_table};"
        logger.info(f"Executing: {truncate_query}")

        glueContext.write_dynamic_frame.from_jdbc_conf(
            frame=dynamic_frame,
            catalog_connection="Redshift connection_hcd-dev-db",
            connection_options={
                "dbtable": f"{config['REDSHIFT_SCHEMA']}.{redshift_table}",
                "database": config["REDSHIFT_DATABASE"],
                "preactions": truncate_query
            },
            redshift_tmp_dir=config['REDSHIFT_TEMP_DIR']
        )
    except Exception as e:
        logger.error(f"Error processing {folder}: {str(e)}")
        raise

# Process Each Folder
for folder, redshift_table in folders:
    process_folder(folder, redshift_table, announcements_schema)

# Commit Job
job.commit()
