import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.context import SparkContext
from pyspark.sql.functions import col, to_timestamp, when
from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, BooleanType, TimestampType

## Get job parameters
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

## Configurations
s3_certificate_input_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/certificates/"
redshift_connection = "hcd_dev_redshift_connection"
redshift_temp_dir = "s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/"
redshift_database = "hcd-dev-db"
redshift_certificate_table = "usastaffing_staging.certificate"

## Initialize contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

## Define certificate schema (aligned with Redshift)
certificate_schema = StructType([
    StructField("tenantId", IntegerType(), False),
    StructField("rankingListId", IntegerType(), False),
    StructField("certificateNumber", StringType(), True),
    StructField("certificateType", StringType(), True),
    StructField("certificateStatus", StringType(), True),
    StructField("certificateOrder", StringType(), True),
    StructField("certificatePriorityOrder", StringType(), True),
    StructField("applicantListName", StringType(), True),
    StructField("signedDateTime", TimestampType(), True),
    StructField("issueDateTime", TimestampType(), True),
    StructField("issuer", StringType(), True),
    StructField("certificateAuditedFlag", BooleanType(), True),
    StructField("initialAuditCompleteDateTime", TimestampType(), True),
    StructField("finalAuditCompleteDateTime", TimestampType(), True),
    StructField("auditedBy", StringType(), True),
    StructField("referMethod", StringType(), True),
    StructField("referMethodNumber", FloatType(), True),
    StructField("certificateAmendedFlag", BooleanType(), True),
    StructField("certificateCancelledFlag", BooleanType(), True),
    StructField("certificateExpiredFlag", BooleanType(), True),
    StructField("certificateExpirationDate", TimestampType(), True),
    StructField("ctapictapWellQualifiedScore", FloatType(), True),
    StructField("rankBy", StringType(), True),
    StructField("tieBreaker", StringType(), True),
    StructField("candidateInventoryEnabledFlag", BooleanType(), True),
    StructField("candidateInventoryStartDate", TimestampType(), True),
    StructField("candidateInventoryEndDate", TimestampType(), True),
    StructField("lastModifiedDateTime", TimestampType(), True),
    StructField("dwLastModifiedDateTime", TimestampType(), True)
])

## Process data function
def process_data(input_path, schema, table_name, connection, temp_dir):
    try:
        # Read the CSV into a DataFrame
        df = spark.read.option("header", "true") \
            .option("delimiter", ",") \
            .schema(schema) \
            .csv(input_path)
        
        # Debug: Verify DataFrame
        print(f"Schema for {table_name}:")
        df.printSchema()
        print(f"Sample data for {table_name} (raw):")
        df.show(5, truncate=False)
        print(f"Row count before filter: {df.count()}")

        # Hardcoded filter for certificate
        filtered_df = df.filter(
            (col("tenantId").isNotNull()) & (col("rankingListId").isNotNull())
        )

        # Debug: Check filtered data
        print(f"Sample data after filter for {table_name}:")
        filtered_df.show(5, truncate=False)
        print(f"Row count after filter: {filtered_df.count()}")

        # Transform timestamp columns (ensure conversion)
        timestamp_columns = ["signedDateTime", "issueDateTime", "initialAuditCompleteDateTime",
                            "finalAuditCompleteDateTime", "certificateExpirationDate",
                            "lastModifiedDateTime", "dwLastModifiedDateTime",
                            "candidateInventoryStartDate", "candidateInventoryEndDate"]
        for ts_col in timestamp_columns:
            filtered_df = filtered_df.withColumn(ts_col, to_timestamp(col(ts_col), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS"))

        # Cast boolean fields
        boolean_cols = ["certificateAuditedFlag", "certificateAmendedFlag", "certificateCancelledFlag",
                        "certificateExpiredFlag", "candidateInventoryEnabledFlag"]
        for b_col in boolean_cols:
            filtered_df = filtered_df.withColumn(b_col, when(col(b_col) == "true", True)
                                                .when(col(b_col) == "false", False)
                                                .otherwise(None).cast(BooleanType()))

        # Debug: Final transformed data
        print(f"Sample data after transformations for {table_name}:")
        filtered_df.show(5, truncate=False)

        # Convert to DynamicFrame
        dynamic_frame = DynamicFrame.fromDF(filtered_df, glueContext, f"{table_name}_redshift_frame")

        # Explicit mapping to enforce schema
        mappings = [(field.name, field.dataType.simpleString(), field.name, field.dataType.simpleString()) 
                    for field in certificate_schema.fields]
        mapped_frame = ApplyMapping.apply(frame=dynamic_frame, mappings=mappings)

        # Write to Redshift without auto-creation
        glueContext.write_dynamic_frame.from_jdbc_conf(
            frame=mapped_frame,
            catalog_connection=connection,
            connection_options={
                "dbtable": table_name,
                "database": redshift_database,
                "preactions": f"TRUNCATE TABLE {table_name}",
                "createTableIfNotExists": "false"  # Disable dynamic table creation
            },
            redshift_tmp_dir=temp_dir
        )
        print(f"Data successfully loaded into {table_name}")
        
    except Exception as e:
        print(f"Error processing {table_name}: {str(e)}")
        raise

## Process certificate data
process_data(
    s3_certificate_input_path, certificate_schema, redshift_certificate_table,
    redshift_connection, redshift_temp_dir
)

## Commit the job
job.commit()
