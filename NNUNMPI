import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_timestamp
from pyspark.sql.types import IntegerType, LongType, FloatType, BooleanType, StringType, TimestampType
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from awsglue.utils import getResolvedOptions

# Initialize Glue Context
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
spark = SparkSession.builder.config("spark.sql.debug.maxToStringFields", "100").getOrCreate()
glueContext = GlueContext(spark)
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# S3 Path and Redshift Configurations
s3_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/announcements.csv"
redshift_connection = "hcd_dev_redshift_connection"
redshift_temp_dir = "s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/"
redshift_database = "hcd-dev-db"
redshift_schema = "usastaffing_staging"
redshift_table = "announcement"

# Read CSV into DataFrame
df = spark.read.format("csv") \
    .option("header", "true") \
    .option("inferSchema", "false") \
    .option("delimiter", ",") \
    .load(s3_path)

# Explicit Type Casting for Redshift Schema
df = df.withColumn("tennantid", col("tennantid").cast(IntegerType())) \
       .withColumn("announcementId", col("announcementId").cast(LongType())) \
       .withColumn("minimumSalary", col("minimumSalary").cast(FloatType())) \
       .withColumn("maximumSalary", col("maximumSalary").cast(FloatType())) \
       .withColumn("openDate", to_timestamp(col("openDate"), "yyyy-MM-dd HH:mm:ss")) \
       .withColumn("closeDate", to_timestamp(col("closeDate"), "yyyy-MM-dd HH:mm:ss")) \
       .withColumn("releasedFlag", col("releasedFlag").cast(BooleanType())) \
       .withColumn("releaseddatetime", to_timestamp(col("releaseddatetime"), "yyyy-MM-dd HH:mm:ss")) \
       .withColumn("applicationLimit", col("applicationLimit").cast(IntegerType())) \
       .withColumn("lastmodifieddatetime", to_timestamp(col("lastmodifieddatetime"), "yyyy-MM-dd HH:mm:ss")) \
       .withColumn("dwLastModifieddatetime", to_timestamp(col("dwLastModifieddatetime"), "yyyy-MM-dd HH:mm:ss"))

# Handle NULL values for NOT NULL columns
df = df.fillna({
    "tennantid": 0,
    "announcementId": 0,
    "minimumSalary": 0.0,
    "maximumSalary": 0.0,
    "releasedFlag": False,
    "applicationLimit": 0
})

# Convert DataFrame to DynamicFrame for Glue
dynamic_frame = DynamicFrame.fromDF(df, glueContext)

# Write to Redshift
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame=dynamic_frame,
    catalog_connection=redshift_connection,
    connection_options={
        "database": redshift_database,
        "dbtable": f"{redshift_schema}.{redshift_table}",
        "aws_iam_role": "arn:aws:iam::094737541415:role/RedshiftGlueRole",
        "batchsize": "50000"  # Adjust batch size for performance
    },
    redshift_tmp_dir=redshift_temp_dir
)

job.commit()
