import sys
import boto3
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.sql import SparkSession
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame

# Initialize Spark, Glue Context, and Job
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
spark = SparkSession.builder.config("spark.sql.debug.maxToStringFields", "1000").getOrCreate()
glueContext = GlueContext(spark)
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Redshift connection details
redshift_conn_name = "hcd_dev_redshift_connection"
redshift_temp_dir = "s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/"
redshift_db = "hcd-dev-db"
redshift_schema = "usastaffing_staging"
s3_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/"  # Updated S3 path

# Table mappings for S3 folder to Redshift table
table_mappings = {
    "announcements": "announcement",
    "certificateapplications": "certificateapplication",
    "certificates": "certificate",
    "customers": "customer",
    "newhires": "new_hire",
    "offices": "office",
    "organizations": "organization",
    "tth": "tth",
    "requests": "request"
}

# Iterate through each folder and load data into Redshift
for folder, table in table_mappings.items():
    s3_source = f"{s3_path}{folder}/"
    print(f"Processing {s3_source} -> {table}")

    # Read CSV data from S3
    df = spark.read.format("csv") \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .load(s3_source)

    dynamic_frame = DynamicFrame.fromDF(df, glueContext, "dynamic_frame")

    # Truncate Redshift table before loading new data
    redshift_query = f"TRUNCATE TABLE {redshift_schema}.{table};"
    redshift_conn = glueContext.write_dynamic_frame.from_options(
        frame=DynamicFrame.fromDF(spark.sql(f"SELECT '{redshift_query}'"), glueContext, "truncate"),
        connection_type="redshift",
        connection_options={"redshiftTmpDir": redshift_temp_dir, "useConnectionProperties": "true", "connectionName": redshift_conn_name},
    )

    # Load data into Redshift
    glueContext.write_dynamic_frame.from_options(
        frame=dynamic_frame,
        connection_type="redshift",
        connection_options={
            "url": f"jdbc:redshift://{redshift_db}.cluster-xxxxxx.us-gov-west-1.redshift.amazonaws.com:5439/{redshift_db}",
            "dbtable": f"{redshift_schema}.{table}",
            "connectionName": redshift_conn_name,
            "redshiftTmpDir": redshift_temp_dir,
            "aws_iam_role": "project-hcd-glu-role"
        }
    )

print("Data Load Complete")
job.commit()
