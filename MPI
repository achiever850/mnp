import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
import boto3

# Initialize Glue context
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
logger = glueContext.get_logger()

# Redshift connection details
s3_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/"
redshift_connection = "hcd_dev_redshift_connection"  # Glue connection name
redshift_temp_dir = "s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/"
redshift_database = "hcd-dev-db"
redshift_schema = "usastaffing_staging"

# List of folder names to process
folders = [
    "announcements", "certificateapplications", "certificates", "customers",
    "newhires", "offices", "organizations", "tth", "requests"
]

# Loop through each folder and load data into Redshift
for folder in folders:
    table_name = f"{redshift_schema}.{folder}"  # Target Redshift table

    # Read data from S3 CSV files
    s3_source_path = f"{s3_path}{folder}/"
    
    df = glueContext.create_dynamic_frame.from_options(
        format_options={"withHeader": True, "separator": ","},
        connection_type="s3",
        format="csv",
        connection_options={"paths": [s3_source_path], "recurse": True}
    )

    # Convert DynamicFrame to DataFrame for logging
    df_count = df.toDF().count()
    logger.info(f"Loading {df_count} records into {table_name}")

    if df_count > 0:
        # Truncate Redshift table before loading new data
        conn_options = {
            "url": f"jdbc:redshift://{redshift_connection}",
            "dbtable": table_name,
            "redshiftTmpDir": redshift_temp_dir
        }
        
        # Execute Truncate Statement
        spark._jvm.com.amazonaws.services.glue.JDBCWrapper(conn_options).executeUpdate(f"TRUNCATE TABLE {table_name}")

        # Write data to Redshift
        glueContext.write_dynamic_frame.from_jdbc_conf(
            frame=df,
            catalog_connection=redshift_connection,
            connection_options={
                "dbtable": table_name,
                "database": redshift_database
            },
            redshift_tmp_dir=redshift_temp_dir
        )

        logger.info(f"Successfully loaded {df_count} records into {table_name}")
    else:
        logger.warn(f"No records found in {folder}, skipping...")

logger.info("All tables loaded successfully")
