import sys
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import col, to_timestamp, when, lit

## @params: [JOB_NAME]
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Define S3 Path and Redshift configurations
s3_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/announcements"
redshift_connection_name = "hcd_dev_redshift_connection"
redshift_temp_dir = "s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/"
redshift_database = "hcd-dev-db"
redshift_schema = "usastaffing_staging"
redshift_table = "announcement"

# Read data from S3
datasource0 = glueContext.create_dynamic_frame.from_options(
    format_options={'withHeader': True, 'separator': ','},
    connection_type="s3",
    format="csv",
    connection_options={
        "paths": [s3_path],
        "recurse": True
    },
    transformation_ctx="datasource0"
)

# Convert DynamicFrame to DataFrame for easier manipulation
df = datasource0.toDF()

# Rename columns to match Redshift table structure
df = df.withColumnRenamed("tenantId", "tennantid") \
       .withColumnRenamed("announcementId", "announcement Id") \
       .withColumnRenamed("announcementStatus", "announcement status") \
       .withColumnRenamed("linkedUSAJOBSControlNumber", "linkedUSAJOBSCONTROLNUMBER") \
       .withColumnRenamed("whoMayApply", "WhoMayApply")

# Handle required NOT NULL columns
df = df.withColumn("tennantid", 
                   when(col("tennantid").isNull(), lit(0)).otherwise(col("tennantid").cast("int")))
df = df.withColumn("announcement Id", 
                   when(col("announcement Id").isNull(), lit(0)).otherwise(col("announcement Id").cast("bigint")))
df = df.withColumn("announcementNumber", 
                   when(col("announcementNumber").isNull(), lit("")).otherwise(col("announcementNumber").cast("string")))

# Convert string columns to appropriate types
for col_name in ["openDate", "closeDate", "releasedDateTime", "lastModifiedDateTime", "dwLastModifiedDateTime"]:
    df = df.withColumn(col_name, to_timestamp(col(col_name), "yyyy-MM-dd'T'HH:mm:ss.SSS"))

# Convert boolean strings to actual boolean values
for col_name in ["releasedFlag", "applicationLimitSetFlag", "displayDefaultBenefitsTextFlag"]:
    df = df.withColumn(col_name, col(col_name) == "true")

# Convert numeric types
for col_name, target_type in [("minimumSalary", "float"), ("maximumSalary", "float"), ("applicationLimit", "int"),
                              ("externalContactId", "bigint"), ("internalContactId", "bigint"), ("usajobsControlNumber", "bigint")]:
    df = df.withColumn(col_name, col(col_name).cast(target_type))

# Write to Redshift using AWS Glue Connection
df.write \
  .format("jdbc") \
  .option("dbtable", f"{redshift_schema}.{redshift_table}") \
  .option("tempdir", redshift_temp_dir) \
  .option("connectionName", redshift_connection_name) \
  .mode("append") \
  .save()

job.commit()
