import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.context import SparkContext
from pyspark.sql.functions import col, to_timestamp
from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, BooleanType, DateType, DoubleType

## Get job parameters
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

## Configurations
s3_office_input_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/offices/"
s3_organization_input_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/organizations/"
s3_customer_input_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/customers/"
s3_certificate_input_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/certificates/"

redshift_office_table = "usastaffing_staging.office"
redshift_organization_table = "usastaffing_staging.organization"
redshift_customer_table = "usastaffing_staging.customer"
redshift_certificate_table = "usastaffing_staging.certificate"

## Initialize contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

## Define schema for office, organization, customer, and certificate tables

# Define office and organization schema as provided earlier
office_org_schema = StructType([
    StructField("tenantId", IntegerType(), False),
    StructField("officeId", IntegerType(), False),
    StructField("officeName", StringType(), True),
    StructField("city", StringType(), True),
    StructField("state", StringType(), True),
    StructField("country", StringType(), True),
    StructField("phone", StringType(), True),
    StructField("fax", StringType(), True),
    StructField("email", StringType(), True),
    StructField("createdBy", StringType(), True),
    StructField("createdDate", StringType(), True),
    StructField("modifiedBy", StringType(), True),
    StructField("modifiedDate", StringType(), True)
])

# Define customer schema as provided earlier
customer_schema = StructType([
    StructField("tenantId", IntegerType(), False),
    StructField("customerID", IntegerType(), False),
    StructField("customerName", StringType(), True),
    StructField("externalName", StringType(), True),
    StructField("agencycode", StringType(), True),
    StructField("agencyName", StringType(), True),
    StructField("agencyReferenceCode", StringType(), True),
    StructField("departmentCode", StringType(), True),
    StructField("departmentName", StringType(), True),
    StructField("addressLine1", StringType(), True),
    StructField("addressLine2", StringType(), True),
    StructField("addressLine3", StringType(), True),
    StructField("city", StringType(), True),
    StructField("ZipCode", StringType(), True),
    StructField("state", StringType(), True),
    StructField("stateAbbreviation", StringType(), True),
    StructField("country", StringType(), True),
    StructField("countryAbbreviation", StringType(), True),
    StructField("phoneNumber", StringType(), True),
    StructField("faxNumber", StringType(), True),
    StructField("opfInstance", StringType(), True),
    StructField("dwlastmodifiedDateTime", StringType(), True)
])

# Define certificate schema as provided earlier
certificate_schema = StructType([
    StructField("tenantId", IntegerType(), False),
    StructField("rankingListId", IntegerType(), False),
    StructField("certificateNumber", StringType(), True),
    StructField("certificateType", StringType(), True),
    StructField("certificateStatus", StringType(), True),
    StructField("certificateOrder", StringType(), True),
    StructField("certificatepriorityOrder", StringType(), True),
    StructField("applicantListName", StringType(), True),
    StructField("signedDateTime", StringType(), True),
    StructField("issueDateTime", StringType(), True),
    StructField("issuer", StringType(), False),
    StructField("certificateAuditedFlag", BooleanType(), True),
    StructField("initialAuditCompleteDateTime", StringType(), True),
    StructField("finalAuditCompleteDateTime", StringType(), True),
    StructField("auditedBy", StringType(), True),
    StructField("refermethod", StringType(), True),
    StructField("refermethodNumber", DoubleType(), True),
    StructField("certificateAmendedFlag", BooleanType(), True),
    StructField("certificatecancelledFlag", BooleanType(), True),
    StructField("certificateExpiredFlag", BooleanType(), True),
    StructField("certificateExpirationDate", StringType(), True),
    StructField("ctapictapwellQualifiedScore", DoubleType(), True),
    StructField("rankBy", StringType(), True),
    StructField("tieBreaker", StringType(), True),
    StructField("candidateInventoryEnabledFlag", BooleanType(), True),
    StructField("candidateInventoryStartDate", StringType(), True),
    StructField("candidateInventoryEndDate", StringType(), True),
    StructField("lastModifiedDateTime", StringType(), True),
    StructField("dwLastModifiedDateTime", StringType(), True)
])

## Read and process office, organization, customer, and certificate data

# Office data processing
office_df = spark.read.option("header", "true").option("delimiter", ",").schema(office_org_schema).csv(s3_office_input_path)
office_df = office_df.withColumn("dwlastmodifiedDateTime", to_timestamp(col("dwlastmodifiedDateTime"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS"))

# Organization data processing
organization_df = spark.read.option("header", "true").option("delimiter", ",").schema(office_org_schema).csv(s3_organization_input_path)
organization_df = organization_df.withColumn("dwlastmodifiedDateTime", to_timestamp(col("dwlastmodifiedDateTime"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS"))

# Customer data processing
customer_df = spark.read.option("header", "true").option("delimiter", ",").schema(customer_schema).csv(s3_customer_input_path)
customer_df = customer_df.withColumn("dwlastmodifiedDateTime", to_timestamp(col("dwlastmodifiedDateTime"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS"))

# Certificate data processing
certificate_df = spark.read.option("header", "true").option("delimiter", ",").schema(certificate_schema).csv(s3_certificate_input_path)
certificate_df = certificate_df.withColumn("signedDateTime", to_timestamp(col("signedDateTime"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS")) \
    .withColumn("issueDateTime", to_timestamp(col("issueDateTime"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS")) \
    .withColumn("certificateExpirationDate", to_timestamp(col("certificateExpirationDate"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS")) \
    .withColumn("candidateInventoryStartDate", to_timestamp(col("candidateInventoryStartDate"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS")) \
    .withColumn("candidateInventoryEndDate", to_timestamp(col("candidateInventoryEndDate"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS")) \
    .withColumn("lastModifiedDateTime", to_timestamp(col("lastModifiedDateTime"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS")) \
    .withColumn("dwLastModifiedDateTime", to_timestamp(col("dwLastModifiedDateTime"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS"))

## Write data to Redshift

# Write Office data to Redshift
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame=DynamicFrame.fromDF(office_df, glueContext, "office_redshift_frame"),
    catalog_connection=redshift_connection,
    connection_options={
        "dbtable": redshift_office_table,
        "database": redshift_database,
        "preactions": f"TRUNCATE TABLE {redshift_office_table}"
    },
    redshift_tmp_dir=redshift_temp_dir
)

# Write Organization data to Redshift
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame=DynamicFrame.fromDF(organization_df, glueContext, "organization_redshift_frame"),
    catalog_connection=redshift_connection,
    connection_options={
        "dbtable": redshift_organization_table,
        "database": redshift_database,
        "preactions": f"TRUNCATE TABLE {redshift_organization_table}"
    },
    redshift_tmp_dir=redshift_temp_dir
)

# Write Customer data to Redshift
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame=DynamicFrame.fromDF(customer_df, glueContext, "customer_redshift_frame"),
    catalog_connection=redshift_connection
