import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.context import SparkContext
from pyspark.sql.functions import col, to_timestamp, when, regexp_replace, substring, length, lit
from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, BooleanType, TimestampType

## Get job parameters
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

## Configurations
s3_newhires_input_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/newhires/"
redshift_connection = "hcd_dev_redshift_connection"
redshift_temp_dir = "s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/"
redshift_database = "hcd-dev-db"
redshift_newhires_table = "usastaffing_staging.newhire"

## Initialize contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

## Define newhire schema (aligned with Redshift DDL)
newhires_schema = StructType([
    StructField("tenantId", IntegerType(), False),
    StructField("newHireId", LongType(), False),
    StructField("newHireNumber", StringType(), True),
    StructField("newHireStatus", StringType(), True),
    StructField("firstName", StringType(), True),
    StructField("middleName", StringType(), True),
    StructField("lastName", StringType(), True),
    StructField("suffix", StringType(), True),
    StructField("newHireName", StringType(), True),
    StructField("email", StringType(), True),
    StructField("addressLine1", StringType(), True),
    StructField("addressLine2", StringType(), True),
    StructField("aptSuite", StringType(), True),
    StructField("city", StringType(), True),
    StructField("state", StringType(), True),
    StructField("zipCode", StringType(), True),
    StructField("country", StringType(), True),
    StructField("creationDate", TimestampType(), True),
    StructField("appointmentType", StringType(), True),
    StructField("effectiveDateofAppointment", TimestampType(), True),
    StructField("projectedStartDate", TimestampType(), True),
    StructField("prolongedStartDateReason", StringType(), True),
    StructField("actualStartDate", TimestampType(), True),
    StructField("newHireHyperlink", StringType(), True),
    StructField("firstLoggedInDate", TimestampType(), True),
    StructField("lastLoggedInDate", TimestampType(), True),
    StructField("onboardingProcessOwner", StringType(), True),
    StructField("onboardingProcessOwnerEmail", StringType(), True),
    StructField("supervisorManager", StringType(), True),
    StructField("supervisorManagerEmail", StringType(), True),
    StructField("eVerifyCaseNumber", StringType(), True),
    StructField("eVerifyFlag", BooleanType(), True),
    StructField("pcsEligibleFlag", BooleanType(), True),
    StructField("adjudicatedVeteransPreferenceCode", StringType(), True),
    StructField("adjudicatedVeteransPreferenceDescription", StringType(), True),
    StructField("activityUnit", StringType(), True),
    StructField("branchOrganization", StringType(), True),
    StructField("bureauDivision", StringType(), True),
    StructField("agencyDepartment", StringType(), True),
    StructField("agencyDepartmentCity", StringType(), True),
    StructField("agencyDepartmentState", StringType(), True),
    StructField("agencyDepartmentZipCode", StringType(), True),
    StructField("agencyDepartmentCountry", StringType(), True),
    StructField("agencyDepartmentAddressLine1", StringType(), True),
    StructField("agencyDepartmentAddressLine2", StringType(), True),
    StructField("agencyDepartmentAddressLine3", StringType(), True),
    StructField("dutyLocationAddressLine1", StringType(), True),
    StructField("dutyLocationAddressLine2", StringType(), True),
    StructField("dutyLocationCity", StringType(), True),
    StructField("dutyLocationState", StringType(), True),
    StructField("dutyLocationCountry", StringType(), True),
    StructField("positionDescriptionNumber", StringType(), True),
    StructField("positionDescriptionTitle", StringType(), True),
    StructField("positionDescriptionSeries", StringType(), True),
    StructField("positionDescriptionPayPlan", StringType(), True),
    StructField("positionDescriptionGrade", StringType(), True),
    StructField("payPlanSeriesGrade", StringType(), True),
    StructField("lastUpdatedDateTime", TimestampType(), True),
    StructField("dwLastModifiedDateTime", TimestampType(), True)
])

## Process data function (exact match to certificate with fixes)
def process_data(input_path, schema, table_name, connection, temp_dir):
    try:
        # Read the CSV into a DataFrame
        df = spark.read.option("header", "true") \
            .option("delimiter", ",") \
            .schema(schema) \
            .csv(input_path)
        
        # Debug: Verify DataFrame and available columns
        print(f"Schema for {table_name}:")
        df.printSchema()
        print(f"Available columns in DataFrame: {df.columns}")
        print(f"Sample data for {table_name} (raw):")
        df.show(5, truncate=False)
        print(f"Row count before filter: {df.count()}")

        # Hardcoded filter for newhire
        filtered_df = df.filter(
            (col("tenantId").isNotNull()) & 
            (col("newHireId").isNotNull())
        )

        # Check if columns exist before applying transformations
        available_cols = [col.lower() for col in filtered_df.columns]
        
        # Clean columns: Remove whitespace and truncate to DDL lengths
        if "positiondescriptiongrade" in available_cols:
            filtered_df = filtered_df.withColumn(
                "positionDescriptionGrade", substring(regexp_replace(col("positionDescriptionGrade"), r"\s+", ""), 1, 10)
            )
        if "positiondescriptionpayplan" in available_cols:
            filtered_df = filtered_df.withColumn(
                "positionDescriptionPayPlan", substring(regexp_replace(col("positionDescriptionPayPlan"), r"\s+", ""), 1, 10)
            )
        if "positiondescriptionseries" in available_cols:
            filtered_df = filtered_df.withColumn(
                "positionDescriptionSeries", substring(regexp_replace(col("positionDescriptionSeries"), r"\s+", ""), 1, 10)
            )
        if "agencydepartmentaddressline1" in available_cols:
            filtered_df = filtered_df.withColumn(
                "agencyDepartmentAddressLine1", substring(regexp_replace(col("agencyDepartmentAddressLine1"), r"\s+$", ""), 1, 150)
            )
        if "agencydepartmentaddressline2" in available_cols:
            filtered_df = filtered_df.withColumn(
                "agencyDepartmentAddressLine2", substring(regexp_replace(col("agencyDepartmentAddressLine2"), r"\s+$", ""), 1, 150)
            )
        if "agencydepartmentaddressline3" in available_cols:
            filtered_df = filtered_df.withColumn(
                "agencyDepartmentAddressLine3", substring(regexp_replace(col("agencyDepartmentAddressLine3"), r"\s+$", ""), 1, 150)
            )
        if "agencydepartmentcity" in available_cols:
            filtered_df = filtered_df.withColumn(
                "agencyDepartmentCity", substring(regexp_replace(col("agencyDepartmentCity"), r"\s+$", ""), 1, 150)
            )
        if "agencydepartmentstate" in available_cols:
            filtered_df = filtered_df.withColumn(
                "agencyDepartmentState", substring(regexp_replace(col("agencyDepartmentState"), r"\s+", ""), 1, 50)
            )
        if "agencydepartmentzipcode" in available_cols:
            filtered_df = filtered_df.withColumn(
                "agencyDepartmentZipCode", substring(regexp_replace(col("agencyDepartmentZipCode"), r"\s+", ""), 1, 50)
            )
        if "agencydepartmentcountry" in available_cols:
            filtered_df = filtered_df.withColumn(
                "agencyDepartmentCountry", substring(regexp_replace(col("agencyDepartmentCountry"), r"\s+$", ""), 1, 150)
            )
        if "dutylocationaddressline1" in available_cols:
            filtered_df = filtered_df.withColumn(
                "dutyLocationAddressLine1", substring(regexp_replace(col("dutyLocationAddressLine1"), r"\s+$", ""), 1, 300)
            )
        if "dutylocationaddressline2" in available_cols:
            filtered_df = filtered_df.withColumn(
                "dutyLocationAddressLine2", substring(regexp_replace(col("dutyLocationAddressLine2"), r"\s+$", ""), 1, 300)
            )
        if "dutylocationcity" in available_cols:
            filtered_df = filtered_df.withColumn(
                "dutyLocationCity", substring(regexp_replace(col("dutyLocationCity"), r"\s+$", ""), 1, 100)
            )
        if "dutylocationstate" in available_cols:
            filtered_df = filtered_df.withColumn(
                "dutyLocationState", substring(regexp_replace(col("dutyLocationState"), r"\s+", ""), 1, 50)
            )
        if "dutylocationcountry" in available_cols:
            filtered_df = filtered_df.withColumn(
                "dutyLocationCountry", substring(regexp_replace(col("dutyLocationCountry"), r"\s+$", ""), 1, 100)
            )
        if "positiondescriptionnumber" in available_cols:
            filtered_df = filtered_df.withColumn(
                "positionDescriptionNumber", substring(regexp_replace(col("positionDescriptionNumber"), r"\s+$", ""), 1, 500)
            )
        if "positiondescriptiontitle" in available_cols:
            filtered_df = filtered_df.withColumn(
                "positionDescriptionTitle", substring(regexp_replace(col("positionDescriptionTitle"), r"\s+$", ""), 1, 200)
            )
        if "payplanseriesgrade" in available_cols:
            filtered_df = filtered_df.withColumn(
                "payPlanSeriesGrade", substring(regexp_replace(col("payPlanSeriesGrade"), r"\s+", ""), 1, 20)
            )

        # Debug: Check filtered data with lengths for affected columns
        print(f"Sample data after filter for {table_name} (with lengths):")
        filtered_df.select(
            "agencyDepartmentAddressLine1" if "agencydepartmentaddressline1" in available_cols else lit(None).alias("agencyDepartmentAddressLine1"),
            length("agencyDepartmentAddressLine1").alias("addr1_len") if "agencydepartmentaddressline1" in available_cols else lit(None).alias("addr1_len"),
            "agencyDepartmentAddressLine2" if "agencydepartmentaddressline2" in available_cols else lit(None).alias("agencyDepartmentAddressLine2"),
            length("agencyDepartmentAddressLine2").alias("addr2_len") if "agencydepartmentaddressline2" in available_cols else lit(None).alias("addr2_len"),
            "agencyDepartmentAddressLine3" if "agencydepartmentaddressline3" in available_cols else lit(None).alias("agencyDepartmentAddressLine3"),
            length("agencyDepartmentAddressLine3").alias("addr3_len") if "agencydepartmentaddressline3" in available_cols else lit(None).alias("addr3_len"),
            "agencyDepartmentCity", length("agencyDepartmentCity").alias("city_len"),
            "agencyDepartmentState", length("agencyDepartmentState").alias("dept_state_len"),
            "agencyDepartmentZipCode", length("agencyDepartmentZipCode").alias("zip_len"),
            "agencyDepartmentCountry", length("agencyDepartmentCountry").alias("country_len"),
            "dutyLocationAddressLine1", length("dutyLocationAddressLine1").alias("duty_addr1_len"),
            "dutyLocationAddressLine2", length("dutyLocationAddressLine2").alias("duty_addr2_len"),
            "dutyLocationCity", length("dutyLocationCity").alias("duty_city_len"),
            "dutyLocationState", length("dutyLocationState").alias("duty_state_len"),
            "dutyLocationCountry", length("dutyLocationCountry").alias("duty_country_len"),
            "positionDescriptionNumber", length("positionDescriptionNumber").alias("pd_num_len"),
            "positionDescriptionTitle", length("positionDescriptionTitle").alias("pd_title_len"),
            "positionDescriptionPayPlan", length("positionDescriptionPayPlan").alias("payplan_len"),
            "positionDescriptionSeries", length("positionDescriptionSeries").alias("series_len"),
            "positionDescriptionGrade", length("positionDescriptionGrade").alias("grade_len"),
            "payPlanSeriesGrade", length("payPlanSeriesGrade").alias("pay_grade_len"),
            "lastUpdatedDateTime",
            "dwLastModifiedDateTime"
        ).show(5, truncate=False)
        print(f"Row count after filter: {filtered_df.count()}")

        # Validate agencyDepartmentState length before Redshift load
        oversized_states = filtered_df.filter(length(col("agencyDepartmentState")) > 50)
        if oversized_states.count() > 0:
            print("WARNING: Found agencyDepartmentState values exceeding 50 characters:")
            oversized_states.select("agencyDepartmentState", length("agencyDepartmentState").alias("state_length")).show(truncate=False)

        # Transform timestamp columns with flexible parsing
        timestamp_columns = [
            "creationDate", "effectiveDateofAppointment", "projectedStartDate", "actualStartDate",
            "firstLoggedInDate", "lastLoggedInDate", "lastUpdatedDateTime", "dwLastModifiedDateTime"
        ]
        for ts_col in timestamp_columns:
            filtered_df = filtered_df.withColumn(
                ts_col,
                when(to_timestamp(col(ts_col), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS").isNotNull(),
                     to_timestamp(col(ts_col), "yyyy-MM-dd'T'HH:mm:ss.SSSSSSS"))
                .otherwise(to_timestamp(col(ts_col), "yyyy-MM-dd HH:mm:ss"))
            )

        # Cast boolean fields
        boolean_cols = ["eVerifyFlag", "pcsEligibleFlag"]
        for b_col in boolean_cols:
            filtered_df = filtered_df.withColumn(b_col, when(col(b_col) == "true", True)
                                                .when(col(b_col) == "false", False)
                                                .otherwise(None).cast(BooleanType()))

        # Debug: Final transformed data
        print(f"Sample data after transformations for {table_name}:")
        filtered_df.show(5, truncate=False)

        # Convert to DynamicFrame
        dynamic_frame = DynamicFrame.fromDF(filtered_df, glueContext, f"{table_name}_redshift_frame")

        # Explicit mapping to enforce schema
        mappings = [(field.name, field.dataType.simpleString(), field.name, field.dataType.simpleString()) 
                    for field in schema.fields]
        mapped_frame = ApplyMapping.apply(frame=dynamic_frame, mappings=mappings)

        # Write to Redshift
        glueContext.write_dynamic_frame.from_jdbc_conf(
            frame=mapped_frame,
            catalog_connection=connection,
            connection_options={
                "dbtable": table_name,
                "database": redshift_database,
                "preactions": f"TRUNCATE TABLE {table_name}",
                "createTableIfNotExists": "false",
                "redshiftTmpDir": temp_dir
            }
        )
        print(f"Data successfully loaded into {table_name}")
        
    except Exception as e:
        print(f"Error processing {table_name}: {str(e)}")
        raise

## Process newhire data
process_data(
    s3_newhires_input_path, newhires_schema, redshift_newhires_table,
    redshift_connection, redshift_temp_dir
)

## Commit the job
job.commit()
