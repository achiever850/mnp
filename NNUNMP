import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import col
from pyspark.sql.types import IntegerType, LongType, BooleanType, FloatType, TimestampType, StringType

# Initialize Glue and Spark Contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# Mapping of S3 folders to Redshift tables
s3_base_path = "s3://hcd-ec2-windows-servers-file-transfer-bucket/usa_staffing_csv/"
table_mappings = {
    "announcements": "announcement"
}

# Redshift Config
redshift_connection = "hcd_dev_redshift_connection"
redshift_database = "hcd-dev-db"
redshift_schema = "usastaffing_staging"
redshift_temp_dir = "s3://aws-glue-assets-094737541415-us-gov-west-1/temporary/"

# Iterate through each folder-table mapping
for s3_folder, redshift_table in table_mappings.items():
    s3_path = f"{s3_base_path}{s3_folder}/"

    # Read CSV into Spark DataFrame
    df = spark.read.option("header", "true").csv(s3_path)

    # Cast columns to match Redshift schema (modify as needed)
    df = df.withColumn("tennantid", col("tennantid").cast(IntegerType())) \
           .withColumn("announcementId", col("announcementId").cast(LongType())) \
           .withColumn("minimumSalary", col("minimumSalary").cast(FloatType())) \
           .withColumn("maximumSalary", col("maximumSalary").cast(FloatType())) \
           .withColumn("openDate", col("openDate").cast(TimestampType())) \
           .withColumn("closeDate", col("closeDate").cast(TimestampType())) \
           .withColumn("releasedFlag", col("releasedFlag").cast(BooleanType())) \
           .withColumn("applicationLimit", col("applicationLimit").cast(IntegerType())) \
           .withColumn("lastmodifieddatetime", col("lastmodifieddatetime").cast(TimestampType())) \
           .withColumn("dwLastModifieddatetime", col("dwLastModifieddatetime").cast(TimestampType()))

    # Convert to Glue DynamicFrame
    dynamic_frame = DynamicFrame.fromDF(df, glueContext, "dynamic_frame")

    # Write to Redshift
    glueContext.write_dynamic_frame.from_jdbc_conf(
        frame=dynamic_frame,
        catalog_connection=redshift_connection,
        connection_options={
            "database": redshift_database,
            "dbtable": f"{redshift_schema}.{redshift_table}",
            "aws_iam_role": "arn:aws:iam::094737541415:role/RedshiftGlueRole",
        },
        redshift_tmp_dir=redshift_temp_dir
    )

    print(f"Data loaded successfully from {s3_path} to Redshift table {redshift_schema}.{redshift_table}")

print("All data loads completed successfully!")
